{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to PySpark\n",
    "\n",
    "Immagine you are interested in working on a large dataset, at the scale of Petabytes:\n",
    "\n",
    "* Data are distributed in many nodes, collecting all of them in one node would be impossible\n",
    "\n",
    "* You want to first load data and explore them\n",
    "\n",
    "* Before moving to machine learning you need to preprocess data\n",
    "\n",
    "What you would like to do is run the preprocessing on each node and return only some high level quantities that will be used in the analysis. \n",
    "\n",
    "This is the core concept of Hadoop and Spark: the code goes to the data rather than the data coming to the machine where the code resides. \n",
    "\n",
    "\n",
    "## Standalone Cluster\n",
    "\n",
    "For this set of lectured we will work with a standalone cluster deployed on your machine. You can use it to develop and test your application before deploying it on a large cluster.\n",
    "\n",
    "<center><img src='imgs/lecture1/standalone_cluster.png'/></center>\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Download [spark](https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz) \n",
    "\n",
    "* Extract the spark tar with `tar xvf spark-3.1.1-bin-hadoop3.2.tgz`\n",
    "\n",
    "* Spark 3 should run on both Java 8/11, so make sure that you have one of them as default (or `JAVA_HOME` environment variable pointing to your java installation)\n",
    "\n",
    "* If this is set correctly, typing `java -version` on your terminal shoud display something like `openjdk version \"1.8.0_252\"`\n",
    "\n",
    "### Create a standalone cluster\n",
    "\n",
    "We will now create a Spark standalone cluster on our local machine. More on this can be found in the [spark documentation](https://spark.apache.org/docs/latest/spark-standalone.html). \n",
    "\n",
    "First we need to start the master: to do this, move into the spark directory, i.e. `cd spark-3.1.1-bin-hadoop3.2`. From here run the following command \n",
    "```\n",
    "./sbin/start-master.sh --host localhost --port 7077 --webui-port 8080\n",
    "```\n",
    "This will spin up the spark master with address `spark://localhost:7077` and a cluster dashboark at `localhost:8080`.\n",
    "\n",
    "We can now create a worker with:\n",
    "\n",
    "```\n",
    "./sbin/start-worker.sh spark://localhost:7077 --cores 2 --memory 2g\n",
    "```\n",
    "\n",
    "After a few seconds it should appear in the master’s web UI. \n",
    "\n",
    "**Note**: ther can be at most one worker per each node, i.e. you can't start two workers. \n",
    "\n",
    "\n",
    "### Running applications\n",
    "\n",
    "We can now submit our application to the cluster with commands such as `spark-submit`. For these sessions we will use jupyter-notebooks to explore pyspark interactively. To do this we need one last package wich will set the environment variables needed to get pyspark modules and use the correct version of python. \n",
    "\n",
    "The package can be installed via pip running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/giulia/anaconda3/lib/python3.8/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/home/giulia/Scrivania/spark-3.1.1-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME=/home/giulia/Scrivania/spark-3.1.1-bin-hadoop3.2/\r\n",
      "PYSPARK_PYTHON=/home/giulia/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "# check some of the env variables\n",
    "!env | grep -i spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Session\n",
    "\n",
    "We can now create the spark session. With the following command we are asking to the master (and the resource manager) to create an application with these configurations. In this case we are using all the default options, e.g. number of core and number of executors, but we can also specify some of them with `.config(\"spark.some.config\", \"value\")`. The list of available configurations can be found [here](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.4.36.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5429147220>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\")\\\n",
    "    .appName(\"First spark application\")\\\n",
    "    .getOrCreate()\n",
    "spark\n",
    "#you can substitute localhost with the huge cluster online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.4.36.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://localhost:7077 appName=First spark application>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get spark context -> entry point used to work with RDD\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize!\n",
    "\n",
    "The first command we will se is `parallelize()`. This function of the `SparkContext` (`sc`) is used to create a Resilient Distributed Datasets (**RDD**) from a list collection. In other words it takes a collection a split it amongst the worker. We will use it to get familiar with the main functions of spark RDDs. \n",
    "\n",
    "Starting from `python_dataset` we will create an RDD, `dist_data`, and then operate on it in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python dataset\n",
    "data = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "# parallelize\n",
    "dist_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... nothing happend in the Web UI! that's because `parallelize` is not an action. We can trigger it using `count`, a function used to count the number of elements in the rdd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count elements of the rdd\n",
    "dist_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using two cores, in the WebUI you will see two parallel lines for this task. This is because the RDD consists of two *partitions*. This is because under the hood RDD are stored as partitions, \"small blocks\" of the original dataset that will be **unit of parallelism**.\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/rdd_partitions.png'/></center><br />\n",
    "\n",
    "\n",
    "In other words if we have two cores in our worker but our dataset is made of one partitions, then only one task at the time will be run. On the other hand, if the partitions are four only two of them will be processed in parallel.\n",
    "\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/partitions_processing.png'/></center><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize using a different number of partitions\n",
    "sc.parallelize(data, numSlices=4)\n",
    "sc.parallelize(data, numSlices=4).getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions\n",
    "dist_data.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `collect()` a distributed dataset is collected in the driver, i.e. all workers will send back the blocks. This is generally used to collect the results of the processing. This function should be used with caution since it fetches the entire RDD to a single machine and can cause the driver to run out of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the data_rdd\n",
    "#it's a pyhton object not an rdd\n",
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and Reduce\n",
    "\n",
    "We can now write our first map-reduce application. the method `map(f)` will apply the function `f` to each element of the rdd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increment each number of the array with map\n",
    "# and collect the result\n",
    "dist_data.map(lambda x: x+1)\n",
    "#he will not do anything: transformation laziness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.map(lambda x: x+1).collect()\n",
    "#to collect the results of our transormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple map transformation\n",
    "dist_data.map(lambda x: x+1).map(lambda x:x/2).collect()\n",
    "#multiple map one after each other: could not have done \n",
    "#it in standard hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce` is an action that aggregates all the elements of the RDD using some function and returns the result to the driver. We will see later on a version of reduce, `reduceByKey`, wich performs the reduction for elements with the same key. `reduceByKey` is not an action since it return a distributed dataset since the result of this aggregation could still be large if the number of keys is large.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increment each number of the dataset and sum \n",
    "# all of them\n",
    "dist_data.map(lambda x: x+1).reduce(lambda x1, x2: x1+x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schematic view of this simple map-reduce application can be seen here:\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/map_reduce_increment.png'/></center><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercize 1\n",
    "\n",
    "Write a reduce function finding the minimum o `data_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.reduce(lambda num1, num2: min(num1, num2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - compute $\\pi$\n",
    "\n",
    "It is possible to estimate $\\pi$ by simulating random points in the unit square (side length of $1$) and counting how many fall in the unit circle. The probability of one point falling inside the circle is \n",
    "\n",
    "$$\n",
    "P = \\frac{\\text{Area circle}}{\\text{Area square}} = \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "We can estimate this probabiliy by counting the number of simulated points inside the circle\n",
    "\n",
    "$$\n",
    "P = \\frac{\\text{#Points in circle}}{\\text{#Points}} \n",
    "$$\n",
    "\n",
    "bbtaining \n",
    "\n",
    "$$\n",
    "\\pi \\approx 4 \\cdot \\frac{\\text{#Points in circle}}{\\text{#Points}}\n",
    "$$\n",
    "\n",
    "<br/><center><img src='imgs/lecture1/pi_estimation.png'/></center><br/>\n",
    "\n",
    "This can be done in Spark with the following steps:\n",
    "1. Create the \"dummy rdd\" containing the placeholders for the points\n",
    "2. Generate the points and check if each of them is inside the circle\n",
    "3. Count the points inside the circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi: 3.1728\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_points = 10000\n",
    "\n",
    "## instantiate \"placeholder RDD\"\n",
    "points_rdd = sc.parallelize([0]*num_points)\n",
    "\n",
    "def in_circle(dummy):\n",
    "    ## simulate the point and check if\n",
    "    ## it is inside the circle \n",
    "    ## return 0 or 1\n",
    "    x=random.random()\n",
    "    y=random.random()\n",
    "    \n",
    "    return 1 if x**2+y**2 <=1 else 0\n",
    "\n",
    "# count points inside the circle\n",
    "count_circle = points_rdd.map(in_circle).reduce(lambda a, b: a+b)\n",
    "\n",
    "# print result\n",
    "print('Pi: {:.4f}'.format(4*count_circle/num_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using the `filter` transformation. `filter(f)` returns a new RDD containing only the element of source RDD on which `f` is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same exercise using filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce by key and flat map \n",
    "\n",
    "Consider the following dataset where each element consists of a tuple `(group, value)` (**key-value pair**). This can be for example group = product class and value = revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [('group1', 10), ('group2', 4), ('group3', 1), ('group2', 7), ('group1', 8)]\n",
    "class_rdd = sc.parallelize(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could be interested in operating only on values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('group1', 11), ('group2', 5), ('group3', 2), ('group2', 8), ('group1', 9)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operate only on the values\n",
    "class_rdd.map(lambda el: (el[0], el[1]+1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be achieved using `mapValues`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same using map values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a reduce function for each class using `reduceByKey`: in this way we are applying the reduce function only to the elements of the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum using reduce by key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not interested in treating the dataset as key-value pairs, or in general we have a non-flat dataset (e.g. each element of the RDD is a list), we can use `flatMap` to \"decompose\" each element. The function of this map must return a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: word count\n",
    "\n",
    "You have received the following message from one of your friend: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ring to rule them all, ? one ring to find them,\n",
      "One ring $ to bring them all, and in the % darkness bind them;\n",
      "In the Land of Mordor @ where the shadows lie.\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    'One ring to rule them all, ? one ring to find them,\\n',\n",
    "    'One ring $ to bring them all, and in the % darkness bind them;\\n',\n",
    "    'In the Land of Mordor @ where the shadows lie.'\n",
    "]\n",
    "\n",
    "print(''.join(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are interested in comping the occurence of each word, but first you need to clean the message (e.g remove symbols such as `@` and `$`) and change all the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One ring to rule them all, ? one ring to find them,\\n',\n",
       " 'One ring $ to bring them all, and in the % darkness bind them;\\n',\n",
       " 'In the Land of Mordor @ where the shadows lie.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parallelize the message\n",
    "message_rdd = sc.parallelize(message)\n",
    "message_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use `string.punctuation` or a regular expression to remove the unwanted character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-2451327474a1>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-2451327474a1>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    if clear_word='':\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# step 1: create the words_rdd where each element is a word\n",
    "# eg: ['one', 'ring', 'to', 'rule', 'them']\n",
    "\n",
    "def clean_row(row):\n",
    "    # clean the row and return the list of words\n",
    "    word_list = []\n",
    "    \n",
    "    ## Code\n",
    "    for word in row.strip().split():\n",
    "        clear_word = ''\n",
    "        for char in word:\n",
    "            if char not in string.punctuation:\n",
    "                clean_word += char\n",
    "        if clear_word='':\n",
    "            word_list.append(clean_word.lower())\n",
    "        \n",
    "    ##\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "# apply transformation\n",
    "words_rdd = message_rdd.map(clean_row)\n",
    "words_rdd.collect()[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: perform word count!\n",
    "# -> crate pairs (word, 1)\n",
    "# -> reduce and count\n",
    "\n",
    "word_count_rdd = \n",
    "word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `takeOrdered(n, key)` allows to collect only the first $n$ elements based on the ordering provided by key. Ordering is by default ascendent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the 3 most occurent words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Linear regression\n",
    "\n",
    "The files in `../datasets/lecture1/` contain the measure of a sensor. Each measure is in the form \n",
    "\n",
    "```\n",
    "Measure  N: (t,val)\n",
    "```\n",
    "\n",
    "There could be pairs such as `(t, None)`, corresponting to missing values. This measure should be removed. \n",
    "\n",
    "We are interested in knowing if there is any sort of relation between `t` and `val`.\n",
    "\n",
    "The first task consits in reading each file and creating an RDD containing as element the pairs `(t,val)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure  0: (-10.00,0.70)\r\n",
      "Measure  1: (-9.95,0.61)\r\n",
      "Measure  2: (-9.90,0.47)\r\n",
      "Measure  3: (-9.85,None)\r\n",
      "Measure  4: (-9.80,0.58)\r\n",
      "Measure  5: (-9.75,0.29)\r\n",
      "Measure  6: (-9.70,0.35)\r\n",
      "Measure  7: (-9.65,0.25)\r\n",
      "Measure  8: (-9.60,0.39)\r\n",
      "Measure  9: (-9.55,0.52)\r\n"
     ]
    }
   ],
   "source": [
    "!head ../datasets/lecture1/file_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that each file contain millions of measure and there may be hundreds of files. Furhermore data can be partitioned in multiple nodes. For this reasons reading all the files with python and then using `parallelize` should be avoided. \n",
    "\n",
    "We can parallelize the list of files and then read them in parallel with spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/giulia/MAPD2021/datasets/lecture1/file_1.txt',\n",
       " '/home/giulia/MAPD2021/datasets/lecture1/file_2.txt',\n",
       " '/home/giulia/MAPD2021/datasets/lecture1/file_3.txt',\n",
       " '/home/giulia/MAPD2021/datasets/lecture1/file_4.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = '/home/giulia/MAPD2021/datasets/lecture1/file_{}.txt'\n",
    "file_list = [base_path.format(i) for i in range(1,5)]\n",
    "\n",
    "files_rdd = sc.parallelize(file_list)\n",
    "files_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "In this case we could have used the function `textFile`, which takes as input the path to the files and read them into an RDD. To access a file in with this function we need to prepend `file://` if the file is in the local file system. If the file is in hadoop the path will be similar to `hdfs://namenode.com:8020/path_to_the_file`. In the same way, if the file is stored in `s3` the path will be `s3://`.\n",
    "\n",
    "The function `textFile` can be used in this case only because files are stored in `.txt` files, and in many cases a custom data loader is required. On the other hand, we should try to use this function as much as possible since they are much more performant than standard python code.  \n",
    "\n",
    "However, in this example we will wire a custom data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Measure  0: (-10.00,0.70)',\n",
       " 'Measure  1: (-9.95,0.61)',\n",
       " 'Measure  2: (-9.90,0.47)',\n",
       " 'Measure  3: (-9.85,None)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd = sc.textFile(\"file:///home/giulia/MAPD2021/datasets/lecture1/file_*.txt\")\n",
    "text_rdd.collect()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the RDD files, write a map that convert it into an RDD, `data_rdd`, where each element is a touple `(t, val)`. Remember to remove points with `None` measure. The elements of `data_rdd` should be something like\n",
    "\n",
    "```\n",
    "[(0.0,9.93), (-1.0,9.02), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_ = 'Measure 0: (-0.6, 8)'\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_file(file):\n",
    "    \n",
    "    ## load lines \n",
    "    ## ...\n",
    "    with open(file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    points = []\n",
    "    ## extract pointsline in line\n",
    "    for line in lines:\n",
    "        float_pattern = ' '\n",
    "        coordinates = re.findall(f'(float_pattern), (float_pattern)', line_)\n",
    "        if len(coordinates)!=0:\n",
    "            coordinates = coordinates[0].split(',')\n",
    "            points.append(\n",
    "            (float)\n",
    "            )\n",
    "    \n",
    "    ## \n",
    "    return points\n",
    "\n",
    "data_rdd = files_rdd.flatMap(lambda file: parse_file(file))\n",
    "data_rdd.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `sample(with_replacement, fraction)` we can sample some points from the original RDD. This is usefull if we want to collect only a fraction of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7242b79177ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$p_x$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "# collect a sample\n",
    "data = np.array(data_rdd.sample(False,0.2).collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "plt.xlabel(r'$p_x$')\n",
    "plt.ylabel(r'$p_y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a distributed gradient descent and use it to find the best parameters for a linear model. Given an input $X$ the ouput of the model is \n",
    "\n",
    "$$\n",
    "Y = W X = w_{0} + w_1 x_1 + \\dots + w_p x_p\n",
    "$$\n",
    "\n",
    "In our case we have $Y=y(w,x)$, $W = [w_0, w_1]$ and $X=[1, x]^T$, in other words\n",
    "\n",
    "$$\n",
    "y(w,x) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "We are interested in estimating the optimal parameters $w^\\star$ of the line fitting our data. To do this we will use gradient descent, an iterative procedure that allows us to find a local minumum of a differentiable function.\n",
    "\n",
    "In our case, we would like to minimize the square residuals, i.e.\n",
    "\n",
    "$$\n",
    "J(W,X) = \\frac{1}{2n} \\sum_{i=1}^{n} [Y(W, X)- y_i]^2 = \\frac{1}{2n} \\sum_{i=1}^{n} [ (w_0 +w_1x) - y_i]^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true value. \n",
    "\n",
    "In each step of the gradient descent we use the following update rule\n",
    "\n",
    "$$\n",
    "W_{i+1} = W_i - \\gamma \\nabla J_W(W_i, X)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is the learning rate, a variable used to reduce the size of each step.\n",
    "In other word we are moving in the opposite direction of the gradient, i.e. towards the minimum of the function. \n",
    "\n",
    "Recalling that $W=[w_0, w_1]$ and $X=[1,x]$ we have that each component of the gradient, i.e. $\\frac{\\partial}{\\partial w_p} J(W,X) $\n",
    "\n",
    "$$\n",
    "\\nabla J(W, X) = \\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot1, \\frac{1}{n} \\sum_{i=1}^{n} [Y(W,X)- y_i]\\cdot x_i \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "We can now write a map-reduce job used to estimate the parameters using gradient descent on the full dataset.\n",
    "\n",
    "We start by defining the weights vector and initializing it to some values, e.g. $(10, 1)$ is a good guess :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement then the functions computing the prediction given as input $x$ and the current weights W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, W):\n",
    "    ## return prediction\n",
    "    \n",
    "\n",
    "# test the function\n",
    "assert predict(1, [10,1]) == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function computing the gradient for one example given as input the point $P=(x,y)$ and the current set of weights $W$. Remember that the gradient has two components, one per parameter. Furthermore the normalization $\\frac{1}{n}$ can be ommited since we can apply it after having summed the gradients of all examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(P, W):\n",
    "    pred = predict(P[0], W)\n",
    "    \n",
    "    ## compute gradient\n",
    "    gradient = \n",
    "    return gradient\n",
    "\n",
    "# test the function\n",
    "assert not (gradient((1,11), [10,1])).all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the gradient descent and find the optimal line parameters. \n",
    "\n",
    "**Hint**: compute the gradient for each point and them sum all of them. Use this sum to update the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count points\n",
    "num_points = \n",
    "\n",
    "# copy weights here: this is usefull if the\n",
    "# cell is run multiple times\n",
    "W = \n",
    "\n",
    "lr = 0.01\n",
    "num_it = 20\n",
    "\n",
    "for i in range(num_it):\n",
    "    ## code\n",
    "    \n",
    "    ##\n",
    "    \n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array(data_rdd.collect())\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "x = np.arange(-10,11)\n",
    "y = W[0] + W[1]*x\n",
    "\n",
    "plt.plot(x, y, color='red', lw=2)\n",
    "\n",
    "plt.xlabel(r'$x_{0}$')\n",
    "plt.ylabel(r'$x_{1}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "From the WebUI we can see that in each iteration Spark is comping everything from the beginning, i.e. the parallelization of the text files. With `cache()` we can tell spark to cache the intermediate results, e.g. after the function parsing the files. \n",
    "\n",
    "In this way the same dataset will be loaded in the next iterations at the cost of having the dataset stored in memory. \n",
    "\n",
    "**Note:** to be precise, there could be different levels of [persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence).\n",
    "\n",
    "The RDD can be unpersisted with `unpersist()`.\n",
    "\n",
    "Performe the gradient descent iterations again by caching `data_rdd` ad look at the WebUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_rdd = data_rdd.persist()\n",
    "\n",
    "## previous code\n",
    "    \n",
    "## \n",
    "\n",
    "data_rdd = data_rdd.unpersist()\n",
    "\n",
    "print(\"Final parameters: x0={:.2f}, x1={:.2f}\".format(W[0], W[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "Compute the residuals usign spark. The residual of the point $(x_i, y_i)$ with respect to the model $y(x)$ is defined as\n",
    "\n",
    "$$\n",
    "R_i = y(x_i) - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_rdd = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals_rdd.collect(), bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/stop-worker.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$SPARK_HOME/sbin/stop-master.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
